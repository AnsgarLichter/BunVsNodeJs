% !TeX root = ..\main.tex
\npchapter{Performanceanalyse} \label{ch:performanceAnalysis}
In diesem Kapitel wird die Performance von Bun ausführlich untersucht und mit Node.js verglichen. Hierbei liegt der Fokus darauf, die Leitfrage \glqq Welche konkreten Leistungsverbesserungen können in Bun 1.0.6 im Vergleich zu Node.js 18.18.2 und 21.0.0 festgestellt werden und wie lassen sie sich quantifizieren?\grqq{} zu beantworten (siehe \autoref{sec:introduction-target}). Zuerst wird die Vorgehensweise bei den Tests vorgestellt. Anschließend wird der verwendete Versuchsaufbau erläutert. Vor der Betrachtung der Ergebnisse folgt die Vorstellung der Beispielimplementierungen.


\section{Vorgehensweise} \label{sec:performance-approach}
Als Metriken für die Performanceanalyse werden die durchschnittliche Latenz, die Anzahl an HTTP-Anfragen pro Sekunde, der Anteil an erfolgreichen HTTP-Anfragen, die CPU-Auslastung und der maximal genutzte Arbeitsspeicher während der Ausführungszeit verwendet. Denn diese spiegeln die in der Theorie erarbeiteten Charakteristiken wider (siehe \autoref{sec:foundations-Performance}). Um die Metriken zu ermitteln, werden verschiedene Testszenarien mit unterschiedlichen Implementierungen betrachtet (siehe \autoref{sec:performance-implementations}). Diese sind auf variierende APIs von Bun und Node.js zurückzuführen. Dadurch werden die Performance der Laufzeitumgebungen selbst bewertet.\\

\noindent
Zuerst erfolgt die Messung der grundlegenden Performance von HTTP-Servern in beiden Laufzeitumgebungen (siehe \autoref{subsec:httpServer}). Dabei greifen 500 gleichzeitige Benutzer für 30 Sekunden auf den Server zu und erhalten einen kurzen Text als Antwort. Dieses Szenario dient der Untersuchung der grundlegenden Netzwerkgeschwindigkeit beider Laufzeitumgebungen. Als Nächstes wird ein Datei-Server aufgesetzt, der jedem Aufrufer ein Bild zurückgibt (siehe \autoref{subsec:fileServer}). Dieser Test wird für 50, 250, 500 und 1.000 gleichzeitige Nutzer über einen Zeitraum von 30 Sekunden durchgeführt. Die Last wird variiert, sodass die Server näher an ihrer Kapazitätsgrenze belastet werden. Der letzte Testfall berechnet die Fibonacci-Folge für die Zahl 45, um die Leistung beider Laufzeitumgebungen bei rechenintensiven Aufgaben zu evaluieren (siehe \autoref{subsec:fibonacci}).

\noindent
Um die Performance korrekt zu bestimmen, müssen geeignete Tools verwendet werden. In dieser Arbeit kommen folgenden Tools zum Einsatz:

\begin{itemize}
	\item Bombardier Version 1.2.6,
	\item GNU Time.
\end{itemize}

\noindent
Bombardier ist ein Werkzeug für die Kommandoziele, das Lasttests über HTTP-Benchmarking ermöglicht. Das Tool eignet sich aufgrund seiner Benutzerfreundlichkeit, der Funktionalitäten und seiner Performance. Es ist in Go geschrieben und verwendet das Paket \glq fasthttp\grq{} anstelle der nativen HTTP-Implementierung von Go. Dadurch ist es ausreichend performant für die Performancetests. Bombardier generiert die HTTP-Anfragen, welche in den ersten beiden Testszenarien an die Server gesendet werden. Mit diesem Tool kann die Dauer der Lasttests sowie die Anzahl der zu versendenden Anfragen konfiguriert werden. Außerdem lässt sich festlegen, wie viele parallele Benutzer simuliert werden. Nach dem Test liefert Bombardier durchschnittliche und maximale Werte für die Anzahl der Anfragen pro Sekunde und die Latenz. Zusätzlich schlüsselt das Tool die erhaltenen HTTP-Statuscodes auf. Dadurch kann der Anteil an erfolgreichen und nicht erfolgreichen Anfragen bestimmt werden.\cite{Fedoseev.2016}\newline
Zur Erfassung der CPU-Auslastung und des maximal genutzten Arbeitsspeichers wird GNU Time verwendet. GNU Time ist auf Ubuntu bereits nativ verfügbar und daher gut geeignet \cite{FreeSoftwareFoundation.2018}. Auf MacOS wird eine entsprechende Portierung dieses Tools genutzt, um vergleichbare Daten zu erheben.\newline
Um möglichst repräsentative Ergebnisse zu erzielen, werden alle Testszenarien für jede Laufzeitumgebung 5-mal wiederholt. Die Durchschnittswerte aus den gesammelten Daten senken die Auswirkung einzelner Abweichungen.


\section{Versuchsaufbau} \label{sec:performance-testSetup}
Um eine konsistente und kontrollierte Umgebung für die Tests zu gewährleisten, werden diese auf spezifischer Hard- und Software durchgeführt. Das Ziel besteht darin, die Testergebnisse reproduzierbar zu gestalten, damit diese unabhängig verifiziert werden können. Aus der Reproduzierbarkeit folgt eine einheitliche Quantifizierung der Ergebnisse zwischen Bun und Node.js. Diese ist notwendig, um die Vergleichbarkeit zu gewährleisten.

\begin{table}[h]
	\caption[Hardware für die Performanceanalyse]{Hardware für die Performanceanalyse\protect\linebreak\textit{Quelle: Eigene Darstellung}}
	\label{table:hardware}
	\centering
	\begin{tabular}{|p{4.5cm}|p{4.5cm}|p{4.5cm}|p{4.5cm}|}
		\hline
		Name & Desktop-PC & MacBook Pro \\
		\hline
		Prozessor & AMD Ryzen 7 2700 @ 3,6 GHz & Apple M1 Pro \\
		\hline
		Arbeitsspeicher & 32 GB DDR4-3200 & 16 GB LPDDR5-6400 \\
		\hline
		Betriebssystem & Ubuntu 23.10 & MacOS 14 Sonoma \\
		\hline
	\end{tabular}
\end{table}

\noindent
\autoref{table:hardware} zeigt die verwendete Hardware und die dazugehörigen Betriebssysteme. Es werden mehrere Betriebssysteme inklusive unterschiedlicher Hardware verwendet. Dadurch kann erkannt werden, ob mögliche Performancevorteile auf eine spezifische Systemumgebung zurückzuführen sind. Die native Implementierung von Bun für Windows ist experimentell und ist für die Öffentlichkeit nicht zugänglich \cite{Verhelst.2023}. Daher ist es nicht möglich, die Funktionsweise von Bun unter Windows zu testen.

\noindent
Die folgenden Versionen der betrachteten Frameworks werden eingesetzt:
\begin{itemize}
	\item Bun Version 1.0.6 (aktuelle Version\footnote{Stand 14.10.2023\label{footnote:Stand}})
	\item Node.js Version 18.18.2 (\ac{lts}\footref{footnote:Stand})
	\item Node.js Version 21.0.0 (aktuelle Version\footref{footnote:Stand})
\end{itemize}

\noindent
Die aktuelle Version von Bun wird für die Tests verwendet, da sie im Vergleich zur Version 1.0 bereits Fehlerkorrekturen enthält \cite{Sumner.2023b}. Bei der Analyse von Node.js werden zwei Versionen einbezogen. Einerseits wird die Version mit \ac{lts} betrachtet, da Node.js diese Version für die meisten Benutzer aufgrund des langfristigen Supports empfiehlt \cite{OpenJSFoundation.o.J.d}. Andererseits enthält die Version 21 von Node.js im Vergleich zur \ac{lts}-Version mehrere Verbesserungen hinsichtlich der Performance \cite{OpenJSFoundation.2023b}. Dazu gehört beispielsweise die neue Version des URL-Parsers Ada. Alleine der aktualisierte URL-Parser verspricht signifikante Performance-Verbesserungen \cite{OpenJSFoundation.2023}.\newline
Der Versuchsaufbau stellt sicher, dass die Performance-Tests zu aussagekräftigen und vergleichbaren Ergebnisse führen und somit der Untersuchung der ersten Forschungsfrage (siehe \autoref{sec:introduction-target}) dienen.

\section{Implementierungen} \label{sec:performance-implementations}
Im Folgenden werden die für jedes Testszenario (siehe \autoref{sec:performance-approach}) verwendeten Implementierungen vorgestellt.

\subsection{HTTP-Server} \label{subsec:httpServer}
Um die grundlegende Performance von Netzwerkanfragen zu bestimmen, werden zwei einfache Programme verwendet. Die Latenz spiegelt die Reaktionszeiten der betrachteten Laufzeitumgebungen wider, da sowohl Client als auch Server auf demselben Endgerät stattfinden und somit keine Verzögerungen durch das Netzwerk auftreten. In \autoref{fig:httpServerBun} ist der Quellcode für Bun dargestellt, während \autoref{fig:httpServerNode} den Quellcode für Node.js zeigt.

\begin{lstlisting}[caption={[HTTP-Server Bun]HTTP-Server Bun\\\textit{Quelle: Eigene Darstellung}},label={fig:httpServerBun}]
	Bun.serve({
		port: 3000,
		fetch(request) {
			return new Response("Hello World!");
		},
	});
\end{lstlisting}

\begin{lstlisting}[caption={[HTTP-Server Node.js]HTTP-Server Node.js\\\textit{Quelle: Eigene Darstellung}},label={fig:httpServerNode}]
	import http from "node:http";
	
	http.createServer(function (request, response) {
		response.write("Hello World!")
		response.end();
	}).listen(3000);
\end{lstlisting}

\noindent
Um die Performance dieser Programme zu testen, werden mit Bombardier 500 gleichzeitige Benutzer für eine Dauer von 30 Sekunden simuliert, wie im Befehl in \autoref{fig:bombardierHttpServer} visualisiert.

\begin{lstlisting}[caption={[Bombardier HTTP-Server]Bombardier HTTP-Server\\\textit{Quelle: Eigene Darstellung}},label={fig:bombardierHttpServer}]
	bombardier -c 500 -d 30s http://localhost:3000
\end{lstlisting}

\noindent
Die Server werden mit der jeweiligen Laufzeitumgebung gestartet. Zur Ermittlung der CPU- und RAM-Auslastung wird GNU Time verwendet. Die Befehle zur Messung der CPU- und RAM-Auslastung sind in Ubuntu und MacOS in den Abbildungen \ref{fig:timeHTTPServerUbuntu} und \ref{fig:timeHTTPServerMacOS} dargestellt. Um dieselben Messungen mit Node.js durchzuführen, muss \glq bun\grq{} durch \glq node\grq{} ersetzt werden.

\begin{lstlisting}[caption={[CPU- und RAM-Messung auf dem Desktop-PC]CPU- und RAM-Messung auf dem Desktop-PC\\\textit{Quelle: Eigene Darstellung}},label={fig:timeHTTPServerUbuntu}]
	/usr/bin/time -f "Execution Time: %e\nMaximum Resident Set Size (RSS): %M\nPercent of CPU This Job Got: %P" bun httpServer.js
\end{lstlisting}

\begin{lstlisting}[caption={[CPU- und RAM-Messung auf dem MacBook Pro]CPU- und RAM-Messung auf dem MacBook Pro\\\textit{Quelle: Eigene Darstellung}},label={fig:timeHTTPServerMacOS}]
	gtime -f "Execution Time: %e\nMaximum Resident Set Size (RSS): %M\nPercent of CPU This Job Got: %P" bun httpServer.js
\end{lstlisting}

\subsection{Datei-Server} \label{subsec:fileServer}
Im zweiten Szenario wird die Übertragung großer Datenmengen geprüft. Dazu werden Bilddateien verwendet. Dieser Prozess inkludiert Zugriffe auf das Dateisystem. Somit wird auch die Performance von diesen getestet. Die Implementierungen für Bun und Node.js sind in den Abbildungen \ref{fig:fileServerBun} und \ref{fig:fileServerNode} dargestellt.

\begin{lstlisting}[caption={[Datei-Server Bun]Datei-Server Bun\\\textit{Quelle: Eigene Darstellung}},label={fig:fileServerBun}]
	const basePath = "../data";
	
	Bun.serve({
		port: 3000,
		fetch(request) {
			const filePath = `${basePath}${new URL(request.url).pathname}`;
			
			try {
				return new Response(Bun.file(filePath));
			} catch (error) {
				return new Response("File not found", {
					status: 404
				});
			}
		},
	});
\end{lstlisting}

\clearpage

\begin{lstlisting}[caption={[Datei-Server Node.js]Datei-Server Node.js\\\textit{Quelle: Eigene Darstellung}},label={fig:fileServerNode}]
	import { createReadStream } from "node:fs";
	import http from "node:http";
	
	const basePath = "../data";
	
	http.createServer((request, response) => {
		const filePath = `${basePath}${request.url}`;
		const readStream = createReadStream(filePath);
		
		readStream.on("open", () => {
			response.setHeader("content-type", "image/png");
			response.writeHead(200);
			
			readStream.pipe(response);
		});
		
		readStream.on("error", () => {
			response.writeHead(404, "Image not found");
			response.end();
		});
	}).listen(3000);
\end{lstlisting}

\noindent
Beide Server rufen dasselbe Bild aus dem Dateisystem ab. Falls unter dem angeforderten Pfad keine Datei gefunden wird, geben beide Server eine entsprechende Fehlermeldung zurück. Dieses Testszenario wird jeweils mit 50, 250, 500 und 1.000 gleichzeitigen Benutzern für eine Dauer von 30 Sekunden getestet (siehe \autoref{sec:performance-approach}). Der Befehl für 50 gleichzeitige Benutzer ist in \autoref{fig:bombardierFileServer} dargestellt. Die Befehle zur Messung der CPU- und RAM-Auslastung unterscheiden sich nicht im Vergleich zum HTTP-Server (siehe \autoref{subsec:httpServer}).

\begin{lstlisting}[caption={[Bombardier Datei-Server]Bombardier Datei-Server\\\textit{Quelle: Eigene Darstellung}},label={fig:bombardierFileServer}]
	bombardier -c 50 -d 30s http://localhost:3000/example.png
\end{lstlisting}

\subsection{Berechnung der Fibonacci-Folge} \label{subsec:fibonacci}
Als letztes Szenario wird die Fibonacci-Folge für die Zahl 45 berechnet, um die Leistung bei der Ausführung rechenintensiver Aufgaben zu bewerten. Hierfür nutzen beide Laufzeitumgebungen die in \autoref{fig:fibonacci} dargestellte Implementierung.

\begin{lstlisting}[caption={[Berechnung der Fibonacci-Folge]Berechnung der Fibonacci-Folge\\\textit{Quelle: Eigene Darstellung}},label={fig:fibonacci}]
	const fibonacci = (number) => {
		if (number <= 0) {
			return 0;
		} else if (number <= 1) {
			return 1;
		} else if (number <= 2) {
			return 2;
		}
		
		return fibonacci(number-1) + fibonacci(number-2);
	};
	
	console.log(fibonacci(45));
\end{lstlisting}

\noindent
Das Programm wird mit beiden Laufzeitumgebungen und GNU Time zur Erhebung der notwendigen Metriken ausgeführt. \autoref{fig:timefibonacciUbuntu} stellt dies beispielsweise für Node.js unter Ubuntu dar. Auf MacOS muss \glq /usr/bin/time\grq{} durch \glq gtime\grq{} ersetzt werden. Um dieselben Messungen mit Bun auszuführen, muss \glq node\grq{} durch \glq bun\grq{} ersetzt werden.
\begin{lstlisting}[caption={[Messung der Fibonacci-Folge auf dem Desktop-PC]Messung der Fibonacci-Folge auf dem Desktop-PC\\\textit{Quelle: Eigene Darstellung}},label={fig:timefibonacciUbuntu}]
	/usr/bin/time -f "Execution Time: %e\nMaximum Resident Set Size (RSS): %M\nPercent of CPU This Job Got: %P" node fibonacci.js
\end{lstlisting}



\section{Ergebnisse} \label{sec:performance-results}
Im Folgenden werden die Ergebnisse der Testszenarien vorgestellt. Im Anschluss folgt die Diskussion über die daraus resultierenden Vor- und Nachteile.\\

\noindent
\textbf{HTTP-Server Performance}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{./images/httpServerAverageRequestsPerSecond.png}
	\caption{HTTP-Server - Durchschnittliche Anzahl an Anfragen pro Sekunde}
	\label{fig:httpServerAverageRequestsPerSecond}
	\textit{Quelle: Eigene Darstellung}
\end{figure}


\noindent
Im ersten Testszenario wird die grundlegende Leistung der HTTP-Server verglichen. \autoref{fig:httpServerAverageRequestsPerSecond} zeigt die durchschnittliche Anzahl an Anfragen pro Sekunde, die jede Laufzeitumgebung beantwortet. Diese wird von Bombardier nach Abschluss des Tests berechnet und ausgegeben. Bun übertrifft auf beiden getesteten Geräten sowohl die LTS-Version als auch die aktuelle Version von Node.js. Bun beantwortet auf dem Desktop-PC pro Sekunde ungefähr 103.000 Anfragen, während Node.js LTS und die aktuelle Version nur 30.000 bzw. 32.000 Anfragen pro Sekunde verarbeiten. Diese Leistungsunterschiede spiegeln sich in den Latenzzeiten wider. Bun hat eine Latenz von ca. 7 ms auf dem MacBook Pro und etwa 5 ms auf dem Desktop-PC, verglichen mit ungefähr 10 ms (MacBook Pro) und etwa 16 ms (Desktop-PC) für Node.js (siehe auch \autoref{fig:httpServerAverageLatency} in \autoref{sec:benchmark-results-diagrams}). Interessant ist, dass die aktuelle Version von Node.js sowohl auf dem MacBook Pro als auch auf dem Desktop-PC eine hohe maximale Latenz aufweist (siehe \autoref{fig:httpServerMaximumLatency} in \autoref{sec:benchmark-results-diagrams}). Auf dem MacBook Pro beträgt die maximale Latenz im Durchschnitt 1,6 Sekunden, auf dem Desktop-PC 8,4 Sekunden. Bun und Node.js \ac{lts} benötigen im Durchschnitt maximal etwa 103 ms bzw. 206 ms zum Beantworten der Anfrage.\newline
Diese Unterschiede deuten darauf hin, dass Bun das Potential bietet, in realen Szenarien schneller zu sein als Node.js. Diese Erkenntnisse legen nahe, dass die Verwendung der JavaScriptCore Engine und der Programmiersprache Zig Vorteile bei der Abarbeitung von Netzwerkanfragen bietet. Das hat Auswirkungen auf die Effizienz und Skalierbarkeit in Produktionsumgebungen.\\

\noindent
\textbf{Datei-Server Performance}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{./images/fileServerAverageLatencyDesktop.png}
	\caption{Datei-Server - Durchschnittliche Latenz}
	\label{fig:fileServerAverageLatency}
	\textit{Quelle: Eigene Darstellung}
\end{figure}

\noindent
Das zweite Testszenario analysiert, inwiefern die Potentiale aus dem ersten Test in einem realen Anwendungsfall umgesetzt werden. Im Folgenden sind die Diagramme für den Desktop-PC zu sehen. Denn die Testergebnisse zeigen keine relevanten Unterschiede zwischen dem Desktop-PC und dem MacBook Pro (siehe \autoref{apx:benchmark-results}). \autoref{fig:fileServerAverageLatency} zeigt die durchschnittlichen Latenzen beim Zugriff auf Bilddateien. Bun schneidet erneut besser ab als Node.js. Bei 50 gleichzeitigen Benutzern beträgt die Latenzzeit für Bun 2 ms, während Node.js LTS und die aktuelle Version Latenzen von 4,2 ms bzw. 3,5 ms aufweisen. Diese Unterschiede werden bei steigender Anzahl gleichzeitiger Benutzer deutlicher. Bei 250 Nutzern beläuft sich Buns Latenz auf ca. 10 ms, der beste Wert von Node.js liegt bei ca. 21 ms (aktuelle Version). Bei 1.000 gleichzeitigen Benutzern benötigt Bun durchschnittlich 42 ms für die Antwort, während die Latenz von Node.js (aktuelle Version) bei 116 ms liegt. Dadurch, dass Bun die Anfragen selbst deutlich schneller verarbeitet, beantwortet Bun deutlich mehr Anfragen pro Sekunde. Trotz der höheren Effizienz bearbeitet Bun alle Anfragen unabhängig von der Anzahl gleichzeitiger Benutzer erfolgreich. In der aktuellen Version von Node.js werden ab 250 gleichzeitigen Benutzern nicht mehr alle Anfragen erfolgreich beantwortet. Bei 1.000 gleichzeitigen Nutzern sinkt die Erfolgsrate auf dem MacBook Pro auf 99,54\%. Bun erzielt bei 50, 250 und 500 gleichzeitigen Benutzern eine hundertprozentige Erfolgsrate bei der Beantwortung der Anfragen. Bei 1.000 simulierten Anwendern sinkt die Erfolgsrate auf 99,96\%. Node.js LTS zeigt ebenfalls eine sehr hohe Erfolgsrate, bei der erst ab 1.000 gleichzeitigen Benutzern fehlerhafte Antworten zurückgeliefert werden. Dabei beträgt die Erfolgsrate 99,92\%. Es ist bemerkenswert, dass die aktuelle Version von Node.js hier Defizite aufweist.\\
\clearpage

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{./images/fileServerRamUsage.png}
	\caption{Datei-Server - Maximal verwendeter Arbeitsspeicher}
	\label{fig:fileServerRamUsage}
	\textit{Quelle: Eigene Darstellung}
\end{figure}


\noindent
Die RAM- und CPU-Nutzung der Laufzeitumgebungen werden bei 1.000 gleichzeitigen Benutzern verglichen. Denn bei der höchsten Last sind die Differenzen am signifikantesten. \autoref{fig:fileServerRamUsage} zeigt den maximal verwendeten Arbeitsspeicher während des 30-sekündigen Tests. Bun ist effizienter als Node.js auf beiden getesteten Geräten. Auf dem MacBook Pro beansprucht Bun etwa 67 MB Arbeitsspeicher, während Node.js LTS und die aktuelle Version etwa 296 MB bzw. 330 MB benötigen. Auf dem Desktop-PC beträgt der Arbeitsspeicherverbrauch von Bun etwa 84 MB, im Vergleich zu 152 MB (LTS) bzw. 212 MB (aktuelle Version) bei Node.js. Die Beobachtungen deuten auf klare Vorteile in Bezug auf Effizienz und Kosten hin. Daraus lässt sich schließen, dass Bun anspruchsvollere Aufgaben mit weniger Ressourcen ausführen kann.\\

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{./images/fileServerCpuUsage.png}
	\caption{Datei-Server - CPU-Auslastung }
	\label{fig:fileServerCpuUsage}
	\textit{Quelle: Eigene Darstellung}
\end{figure}

\noindent
\autoref{fig:fileServerCpuUsage} zeigt die CPU-Auslastung bei 1.000 gleichzeitigen Benutzern in Prozent. Bei der CPU-Auslastung bedeuten 100\%, dass die Summe der Arbeitslast der vollständigen Auslastung eines Kerns entspricht. Wenn ein Computer eine CPU mit vier Kernen besitzt, beträgt die maximale CPU-Auslastung 400\%. Die CPU-Auslastung bestätigt, dass Bun mit den verfügbaren Ressourcen effizienter umgeht als Node.js. Die CPU-Nutzung bei Bun beträgt etwa 91\% auf dem MacBook Pro und 96\% auf dem Desktop-PC. Node.js LTS hingegen benötigt etwa 195\% auf dem MacBook Pro und 167\% auf dem Desktop-PC, während die aktuelle Version schlechter abschneidet. Das heißt, Node.js stellt ungefähr doppelt so starke Anforderungen an die CPU als Bun. Diese Unterschiede in der CPU-Auslastung und Arbeitsspeichernutzung bestärken die Aussage, dass Bun bei gleichzeitiger Last effizienter ist und anspruchsvolle Aufgaben ausführen kann.\\

\noindent
\textbf{Performance bei der Berechnung der Fibonacci-Folge}
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{./images/fibonacciRuntime.png}
	\caption{Berechnung der Fibonacci-Folge - Ausführungszeit}
	\label{fig:fibonacciRuntime}
	\textit{Quelle: Eigene Darstellung}
\end{figure}


\noindent
Der dritte Testfall konzentriert sich auf die Leistung von Bun und Node.js bei der Ausübung rechenintensiver Aufgaben. Die Ausführungszeiten sind in \autoref{fig:fibonacciRuntime} dargestellt. Bun führt die Berechnung durchschnittlich in 3,24 Sekunden auf dem MacBook Pro und in 4,47 Sekunden auf dem Desktop-PC durch. Node.js zeigt kaum Unterschiede zwischen der LTS- und der aktuellen Version, wobei die schnellste Berechnung 6,32 Sekunden (MacBook Pro) und 7,18 Sekunden (Desktop-PC) dauert. Somit ist Bun auf dem MacBook Pro ca. 50\% und auf dem Desktop-PC ungefähr 40\% schneller. Die CPU-Auslastung beträgt sowohl bei Bun als auch bei Node.js ca. 100\% (siehe \autoref{fig:fibonacciCpuUsage} in \autoref{sec:benchmark-results-diagrams}). Der maximal verwendete Arbeitsspeicher unterscheidet sich auf dem Desktop-PC nur um höchstens 3 MB. Erwähnenswert ist, dass hier die aktuelle Version von Node.js 1 MB weniger Arbeitsspeicher als Bun nutzt. Auf dem MacBook Pro sind die Unterschiede größer. Bun benötigt ca. 26 MB RAM, die aktuelle Version von Node.js im Vergleich dazu 37 MB und die LTS-Version 42 MB (siehe \autoref{fig:fibonacciRamUsage} in \autoref{sec:benchmark-results-diagrams}).\newline
Die Betrachtung der CPU-Auslastung, der RAM-Nutzung und der Ausführungszeiten bestätigt die beobachteten Leistungsunterschiede aus den beiden vorherigen Testszenarien. Unabhängig von der betrachteten Metrik weist Bun eine deutlich höhere Leistungsfähigkeit auf beiden Endgeräten auf.

\section{Fazit} \label{sec:performance-conclusion}
Dieses Kapitel beantwortet die erste Leitfrage \glqq Welche konkreten Leistungsverbesserungen können in Bun 1.0.6 im Vergleich zu Node.js 18.18.2 und 21.0.0 festgestellt werden und wie lassen sie sich quantifizieren?\grqq{} (siehe \autoref{ch:introduction}). Der Benchmark verdeutlicht, dass Bun in sämtlichen Testszenarien signifikant bessere Ergenisse als Node.js erzielt hat. Dies zeigt sich in einer reduzierten Latenz und in einer geringeren Inanspruchnahme von Arbeitsspeicher und CPU-Ressourcen während der Verarbeitung von Netzwerkanfragen. Diese Resultate bestätigen sich bei der Ausführung von rechenintensiven Aufgaben. Bun hat die Fibonacci-Folge bis zu 50\% schneller berechnet als Node.js. In diesem Szenario beansprucht Bun vergleichbar viel Arbeitsspeicher wie Node.js, weist jedoch eine geringere CPU-Auslastung auf. \newline
Die Ergebnisse unterstreichen, dass die Entwickler von Bun im Entwicklungsprozess gute Entscheidungen bei der Auswahl der Komponenten getroffen haben. Die JavaScriptCore Engine nutzt ihre mehreren \ac{jit-compiler} (siehe \autoref{sec:foundations-Bun}) für eine effizientere Optimierung des Maschinencodes. Die Auswahl von Zig als systemnahe Programmiersprache und das intensive Testen während der Entwicklung haben (siehe \autoref{sec:foundations-Bun}) die Effizienz von Bun im Umgang mit den verfügbaren Ressourcen gesteigert.\\

\noindent
Der Benchmark basiert auf dem Vergleich ausgewählter Eigenschaften auf einem spezifischen Hardware-Setup. Es ist zu beachten, dass diese Ergebnisse unter Verwendung von Servern in einer Produktivumgebung potenziell abweichen können. Die Analysen beschränken sich auf einfache Beispiele, die sich auf die Performance der Laufzeitumgebungen konzentrieren. In der Realität sind Anwendungsquellcodes oft umfangreicher und komplexer. Die vorliegenden Ergebnisse geben daher nicht zwangsläufig die Performance solcher Anwendungen wieder. Für eine umfassende Bewertung der Performance sollten die Unterschiede zwischen dem MacBook Pro und dem Desktop-PC genauer analysiert werden. Es ist wichtig, die Gründe für die Unterschiede im Anteil erfolgreicher Anfragen zu ermitteln. Die Untersuchung sollte auf andere Hardware-Setups ausgeweitet werden, um die Generalisierbarkeit der Ergebnisse sicherzustellen.